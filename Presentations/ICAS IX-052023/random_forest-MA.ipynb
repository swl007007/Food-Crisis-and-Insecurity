{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import geopandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF (388085, 53)\n"
     ]
    }
   ],
   "source": [
    "# read csv file\n",
    "#df = pd.read_csv(r'C:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\comprehensive_merge.csv')\n",
    "df = pd.read_csv(r'C:\\Users\\MALMANZAR\\CGIAR\\Shi, Weilun (IFPRI) - Food Crisis and Insecurity\\comprehensive_merge.csv')\n",
    "print(\"DF\" ,df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (315776, 18)\n",
      "y (315776,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#drop geometry column\n",
    "df = df.drop(columns=['geometry'])\n",
    "# drop observation with missing phase_worse_percentage_manual\n",
    "df = df.dropna(subset=['phase3_worse_percentage_manual'])\n",
    "# drop columns with 50% more missing values\n",
    "df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
    "# drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# drop columns with all zero values\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "# Assuming 'df' is your dataset as a pandas DataFrame\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[['estimated_population','event_count_battles', 'event_count_explosions', 'event_count_violence',\n",
    "       'fatalities_battles', 'fatalities_explosions', 'fatalities_violence','nearest_neighbor_distance',\n",
    "       'GOSIF_GPP', 'rainfall_chirps', 'GOSIF_GPP_SD', \n",
    "        'temperature_2m_mean', 'temperature_2m_mean_sd',\n",
    "       'shortwave_radiation_sum', 'shortwave_radiation_sum_sd',\n",
    "       'precipitation_sum', 'precipitation_sum_sd','price_index']].values.astype(np.float32)\n",
    "y = df['phase3_plus_phase4'].values.astype(np.float32)\n",
    "\n",
    "print(\"X\" , X.shape)\n",
    "print(\"y\" ,y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# impute X_train_scaled\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X_train_scaled)\n",
    "X_train_scaled = imp.transform(X_train_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# impute X_test_scaled\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X_test_scaled)\n",
    "X_test_scaled = imp.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "# Setting up the RF\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train_scaled, y_train)\n",
    "print(\"Trained\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0001288981881663356\n",
      "Root Mean Squared Error: 0.011353333790844678\n",
      "Mean Absolute Error: 0.0016723832267519678\n",
      "R-squared: 0.9968214771859986\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_regressor.predict(X_test_scaled)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_population: 0.1504907034117259\n",
      "event_count_battles: 0.017185309987697524\n",
      "event_count_explosions: 0.007049468881585583\n",
      "event_count_violence: 0.012437698238063465\n",
      "fatalities_battles: 0.015347084596220216\n",
      "fatalities_explosions: 0.004626226550170163\n",
      "fatalities_violence: 0.011241935044376998\n",
      "nearest_neighbor_distance: 0.05797985285836072\n",
      "GOSIF_GPP: 0.06998317266666618\n",
      "rainfall_chirps: 0.04813349859589511\n",
      "GOSIF_GPP_SD: 0.06975181894096083\n",
      "temperature_2m_mean: 0.08976427111206167\n",
      "temperature_2m_mean_sd: 0.17694545845063994\n",
      "shortwave_radiation_sum: 0.041028453037707475\n",
      "shortwave_radiation_sum_sd: 0.039202429011621534\n",
      "precipitation_sum: 0.031772936570108094\n",
      "precipitation_sum_sd: 0.03182062069957322\n",
      "price_index: 0.12523906134656526\n"
     ]
    }
   ],
   "source": [
    "# Obtain feature importances from the Random Forest model\n",
    "importances = rf_regressor.feature_importances_\n",
    "\n",
    "# Get the feature names from your dataset\n",
    "feature_names = ['estimated_population','event_count_battles', 'event_count_explosions', 'event_count_violence',\n",
    "       'fatalities_battles', 'fatalities_explosions', 'fatalities_violence','nearest_neighbor_distance',\n",
    "       'GOSIF_GPP', 'rainfall_chirps', 'GOSIF_GPP_SD', \n",
    "        'temperature_2m_mean', 'temperature_2m_mean_sd',\n",
    "       'shortwave_radiation_sum', 'shortwave_radiation_sum_sd',\n",
    "       'precipitation_sum', 'precipitation_sum_sd','price_index']\n",
    "\n",
    "# Print the feature importances\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#import pickle\n",
    "#pickle.dump(rf_regressor, open(r'C:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\\\3.Random_Forest_Training\\rf_regressor.pkl', 'wb'))\n",
    "\n",
    "# save scaler\n",
    "#pickle.dump(scaler, open(r'C:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\\\3.Random_Forest_Training\\scaler.pkl', 'wb'))\n",
    "\n",
    "# save test set\n",
    "#np.save(r'C:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\\\3.Random_Forest_Training\\X_test_scaled.npy', X_test_scaled)\n",
    "\n",
    "# save test set\n",
    "#np.save(r'C:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\\\3.Random_Forest_Training\\y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 11610, in _reindex_for_setitem\n",
      "    reindexed_value = value.reindex(index)._values\n",
      "                      ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\series.py\", line 4919, in reindex\n",
      "    return super().reindex(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\", line 5360, in reindex\n",
      "    return self._reindex_axes(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\", line 5375, in _reindex_axes\n",
      "    new_index, indexer = ax.reindex(\n",
      "                         ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 4267, in reindex\n",
      "    indexer = self.get_indexer(\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3801, in get_indexer\n",
      "    return self._get_indexer(target, method, limit, tolerance)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3822, in _get_indexer\n",
      "    tgt_values = engine._extract_level_codes(  # type: ignore[union-attr]\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pandas\\_libs\\index.pyx\", line 714, in pandas._libs.index.BaseMultiIndexCodesEngine._extract_level_codes\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py\", line 142, in _codes_to_ints\n",
      "    codes <<= self.offsets\n",
      "ValueError: operands could not be broadcast together with shapes (78413,2) (3,) (78413,2) \n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\WeilunShi\\AppData\\Local\\Temp\\ipykernel_23836\\3766160736.py\", line 31, in <module>\n",
      "    df_monthly[col + '_12m_avg'] = df_monthly.groupby(level=0)[col].rolling(window=12).mean()\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 3959, in __setitem__\n",
      "    self._set_item(key, value)\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 4152, in _set_item\n",
      "    value = self._sanitize_column(value)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 4875, in _sanitize_column\n",
      "    return _reindex_for_setitem(Series(value), self.index)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 11617, in _reindex_for_setitem\n",
      "    raise TypeError(\n",
      "TypeError: incompatible index of inserted column with frame index\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1071, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1037, in get_records\n",
      "    FIs.append(FrameInfo(\"Raw frame\", filename, lineno, frame, code))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 700, in __init__\n",
      "    ix = inspect.getsourcelines(frame)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\WeilunShi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py\", line 1244, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\WeilunShi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py\", line 1081, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv(r'C:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\comprehensive_merge.csv')\n",
    "#drop geometry column\n",
    "df = df.drop(columns=['geometry'])\n",
    "# drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# drop columns with all zero values\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['area_id', 'date'])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['area_id', 'date'])\n",
    "\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# List of columns you want to calculate rolling average for\n",
    "cols_to_avg = ['event_count_explosions', 'event_count_violence',\n",
    "       'fatalities_battles', 'fatalities_explosions', 'fatalities_violence',\n",
    "       'GOSIF_GPP', 'rainfall_chirps', 'GOSIF_GPP_SD', 'elevation', 'soil',\n",
    "       'price', 'temperature_2m_mean', 'temperature_2m_mean_sd',\n",
    "       'shortwave_radiation_sum', 'shortwave_radiation_sum_sd',\n",
    "       'precipitation_sum', 'precipitation_sum_sd','nearest_neighbor_distance','price_index']\n",
    "\n",
    "df_monthly = df.groupby('area_id')[cols_to_avg].resample('M').mean()\n",
    "\n",
    "for col in cols_to_avg:\n",
    "    df_monthly[col + '_12m_avg'] = df_monthly.groupby(level=0)[col].rolling(window=12).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly.reset_index(inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to datetime\n",
    "df_monthly['date'] = pd.to_datetime(df_monthly['date'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# change date to the first day of the month\n",
    "df_monthly['date'] = df_monthly['date'].dt.strftime('%Y-%m-01')\n",
    "\n",
    "# for each column other than area_id and date in df_monthly, rename the column to column + '_12m_avg'\n",
    "for col in cols_to_avg:\n",
    "    df_monthly[col+'_12_avg'] = df_monthly[col]\n",
    "    df_monthly = df_monthly.drop(columns=[col])\n",
    "\n",
    "\n",
    "# convert date to datetime\n",
    "df_monthly['date'] = pd.to_datetime(df_monthly['date'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# merge df and df_monthly\n",
    "df_new = pd.merge(df, df_monthly, how='left', on=['area_id', 'date'])\n",
    "# drop original columns and rename the columns with _12_avg to the original column name\n",
    "\n",
    "for col in cols_to_avg:\n",
    "    df_new = df_new.drop(columns=[col])\n",
    "    df_new = df_new.rename(columns={col+'_12_avg': col})\n",
    "# drop observation with missing phase_worse_percentage_manual\n",
    "df = df.dropna(subset=['phase3_worse_percentage_manual'])\n",
    "# drop columns with 50% more missing values\n",
    "df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
    "# drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# drop columns with all zero values\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "# Assuming 'df' is your dataset as a pandas DataFrame\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[['estimated_population','event_count_battles', 'event_count_explosions', 'event_count_violence',\n",
    "       'fatalities_battles', 'fatalities_explosions', 'fatalities_violence','nearest_neighbor_distance',\n",
    "       'GOSIF_GPP', 'rainfall_chirps', 'GOSIF_GPP_SD', \n",
    "        'temperature_2m_mean', 'temperature_2m_mean_sd',\n",
    "       'shortwave_radiation_sum', 'shortwave_radiation_sum_sd',\n",
    "       'precipitation_sum', 'precipitation_sum_sd','price_index']].values.astype(np.float32)\n",
    "y = df['phase3_plus_phase4'].values.astype(np.float32)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# impute X_train_scaled\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X_train_scaled)\n",
    "X_train_scaled = imp.transform(X_train_scaled)\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# impute X_test_scaled\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X_test_scaled)\n",
    "X_test_scaled = imp.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.010643312537536807\n",
      "Root Mean Squared Error: 0.10316643125327543\n",
      "Mean Absolute Error: 0.0757753004367654\n",
      "R-squared: 0.7159731425170588\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_regressor.predict(X_test_scaled)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_population: 0.15812652127193444\n",
      "event_count_battles: 0.015005508468965822\n",
      "event_count_explosions: 0.006834930087063135\n",
      "event_count_violence: 0.010777222751171923\n",
      "fatalities_battles: 0.017019351967655173\n",
      "fatalities_explosions: 0.004183655948107346\n",
      "fatalities_violence: 0.012355635093489789\n",
      "nearest_neighbor_distance: 0.05733329083273019\n",
      "GOSIF_GPP: 0.07619310539468775\n",
      "rainfall_chirps: 0.051886844902740724\n",
      "GOSIF_GPP_SD: 0.06938937061676173\n",
      "temperature_2m_mean: 0.08824400867424732\n",
      "temperature_2m_mean_sd: 0.16582877873132315\n",
      "shortwave_radiation_sum: 0.042810250440663306\n",
      "shortwave_radiation_sum_sd: 0.03692621159195161\n",
      "precipitation_sum: 0.03007692512843207\n",
      "precipitation_sum_sd: 0.03184992167797163\n",
      "price_index: 0.125158466420103\n"
     ]
    }
   ],
   "source": [
    "# Obtain feature importances from the Random Forest model\n",
    "importances = rf_regressor.feature_importances_\n",
    "\n",
    "# Get the feature names from your dataset\n",
    "feature_names = ['estimated_population','event_count_battles', 'event_count_explosions', 'event_count_violence',\n",
    "       'fatalities_battles', 'fatalities_explosions', 'fatalities_violence','nearest_neighbor_distance',\n",
    "       'GOSIF_GPP', 'rainfall_chirps', 'GOSIF_GPP_SD', \n",
    "        'temperature_2m_mean', 'temperature_2m_mean_sd',\n",
    "       'shortwave_radiation_sum', 'shortwave_radiation_sum_sd',\n",
    "       'precipitation_sum', 'precipitation_sum_sd','price_index']\n",
    "\n",
    "# Print the feature importances\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv(r'C:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\comprehensive_merge.csv')\n",
    "#drop geometry column\n",
    "df = df.drop(columns=['geometry'])\n",
    "# drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# drop columns with all zero values\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['area_id', 'date'])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['area_id', 'date'])\n",
    "\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# List of columns you want to calculate rolling average for\n",
    "cols_to_avg = ['event_count_explosions', 'event_count_violence',\n",
    "       'fatalities_battles', 'fatalities_explosions', 'fatalities_violence',\n",
    "       'GOSIF_GPP', 'rainfall_chirps', 'GOSIF_GPP_SD', 'elevation', 'soil',\n",
    "       'price', 'temperature_2m_mean', 'temperature_2m_mean_sd',\n",
    "       'shortwave_radiation_sum', 'shortwave_radiation_sum_sd',\n",
    "       'precipitation_sum', 'precipitation_sum_sd','nearest_neighbor_distance','price_index']\n",
    "\n",
    "df_monthly = df.groupby('area_id')[cols_to_avg].resample('M').mean()\n",
    "\n",
    "for col in cols_to_avg:\n",
    "    df_monthly[col + '_12_avg'] = df_monthly.groupby('area_id')[col].rolling(window=6, min_periods=1).mean().reset_index(level=0, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly.reset_index(inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to datetime\n",
    "df_monthly['date'] = pd.to_datetime(df_monthly['date'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# change date to the first day of the month\n",
    "df_monthly['date'] = df_monthly['date'].dt.strftime('%Y-%m-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for each column other than area_id and date in df_monthly, rename the column to column + '_12m_avg'\n",
    "for col in cols_to_avg:\n",
    "    df_monthly = df_monthly.drop(columns=[col])\n",
    "\n",
    "\n",
    "# convert date to datetime\n",
    "df_monthly['date'] = pd.to_datetime(df_monthly['date'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# merge df and df_monthly\n",
    "df_new = pd.merge(df, df_monthly, how='left', on=['area_id', 'date'])\n",
    "# drop original columns and rename the columns with _12_avg to the original column name\n",
    "\n",
    "for col in cols_to_avg:\n",
    "    df_new = df_new.drop(columns=[col])\n",
    "    df_new = df_new.rename(columns={col+'_12_avg': col})\n",
    "# drop observation with missing phase_worse_percentage_manual\n",
    "df = df.dropna(subset=['phase3_worse_percentage_manual'])\n",
    "# drop columns with 50% more missing values\n",
    "df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
    "# drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# drop columns with all zero values\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "# Assuming 'df' is your dataset as a pandas DataFrame\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[['estimated_population','event_count_battles', 'event_count_explosions', 'event_count_violence',\n",
    "       'fatalities_battles', 'fatalities_explosions', 'fatalities_violence','nearest_neighbor_distance',\n",
    "       'GOSIF_GPP', 'rainfall_chirps', 'GOSIF_GPP_SD', \n",
    "        'temperature_2m_mean', 'temperature_2m_mean_sd',\n",
    "       'shortwave_radiation_sum', 'shortwave_radiation_sum_sd',\n",
    "       'precipitation_sum', 'precipitation_sum_sd','price_index']].values.astype(np.float32)\n",
    "y = df['phase3_plus_phase4'].values.astype(np.float32)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# impute X_train_scaled\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X_train_scaled)\n",
    "X_train_scaled = imp.transform(X_train_scaled)\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# impute X_test_scaled\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X_test_scaled)\n",
    "X_test_scaled = imp.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.011379663593989568\n",
      "Root Mean Squared Error: 0.10667550606390189\n",
      "Mean Absolute Error: 0.07561310160731201\n",
      "R-squared: 0.697040778495385\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_regressor.predict(X_test_scaled)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_population: 0.16037584887623985\n",
      "event_count_battles: 0.015292427669536656\n",
      "event_count_explosions: 0.006868191610703205\n",
      "event_count_violence: 0.010725680789604172\n",
      "fatalities_battles: 0.015642792605600055\n",
      "fatalities_explosions: 0.004386836611664744\n",
      "fatalities_violence: 0.011628557570826894\n",
      "nearest_neighbor_distance: 0.05567218937350375\n",
      "GOSIF_GPP: 0.07335559392120494\n",
      "rainfall_chirps: 0.051785111305077965\n",
      "GOSIF_GPP_SD: 0.06653387437862394\n",
      "temperature_2m_mean: 0.08814856741178646\n",
      "temperature_2m_mean_sd: 0.17186510848778094\n",
      "shortwave_radiation_sum: 0.03965378434487858\n",
      "shortwave_radiation_sum_sd: 0.03797881056280124\n",
      "precipitation_sum: 0.03033480184036122\n",
      "precipitation_sum_sd: 0.033098183362711774\n",
      "price_index: 0.1266536392770937\n"
     ]
    }
   ],
   "source": [
    "# Obtain feature importances from the Random Forest model\n",
    "importances = rf_regressor.feature_importances_\n",
    "\n",
    "# Get the feature names from your dataset\n",
    "feature_names = ['estimated_population','event_count_battles', 'event_count_explosions', 'event_count_violence',\n",
    "       'fatalities_battles', 'fatalities_explosions', 'fatalities_violence','nearest_neighbor_distance',\n",
    "       'GOSIF_GPP', 'rainfall_chirps', 'GOSIF_GPP_SD', \n",
    "        'temperature_2m_mean', 'temperature_2m_mean_sd',\n",
    "       'shortwave_radiation_sum', 'shortwave_radiation_sum_sd',\n",
    "       'precipitation_sum', 'precipitation_sum_sd','price_index']\n",
    "\n",
    "# Print the feature importances\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv(r'C:\\Users\\WeilunShi\\OneDrive - CGIAR\\Desktop\\Food Crisis and Insecurity\\comprehensive_merge.csv')\n",
    "#drop geometry column\n",
    "df = df.drop(columns=['geometry'])\n",
    "# drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# drop columns with all zero values\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['area_id', 'date'])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['area_id', 'date'])\n",
    "\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# List of columns you want to calculate rolling average for\n",
    "cols_to_avg = ['event_count_explosions', 'event_count_violence',\n",
    "       'fatalities_battles', 'fatalities_explosions', 'fatalities_violence',\n",
    "       'GOSIF_GPP', 'rainfall_chirps', 'GOSIF_GPP_SD', 'elevation', 'soil',\n",
    "       'price', 'temperature_2m_mean', 'temperature_2m_mean_sd',\n",
    "       'shortwave_radiation_sum', 'shortwave_radiation_sum_sd',\n",
    "       'precipitation_sum', 'precipitation_sum_sd','nearest_neighbor_distance','price_index']\n",
    "\n",
    "df_monthly = df.groupby('area_id')[cols_to_avg].resample('M').mean()\n",
    "\n",
    "for col in cols_to_avg:\n",
    "    df_monthly[col + '_12_avg'] = df_monthly.groupby('area_id')[col].rolling(window=12, min_periods=1).mean().reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly.reset_index(inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to datetime\n",
    "df_monthly['date'] = pd.to_datetime(df_monthly['date'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# change date to the first day of the month\n",
    "df_monthly['date'] = df_monthly['date'].dt.strftime('%Y-%m-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column other than area_id and date in df_monthly, rename the column to column + '_12m_avg'\n",
    "for col in cols_to_avg:\n",
    "    df_monthly = df_monthly.drop(columns=[col])\n",
    "\n",
    "\n",
    "# convert date to datetime\n",
    "df_monthly['date'] = pd.to_datetime(df_monthly['date'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# merge df and df_monthly\n",
    "df_new = pd.merge(df, df_monthly, how='left', on=['area_id', 'date'])\n",
    "# drop original columns and rename the columns with _12_avg to the original column name\n",
    "\n",
    "for col in cols_to_avg:\n",
    "    df_new = df_new.drop(columns=[col])\n",
    "    df_new = df_new.rename(columns={col+'_12_avg': col})\n",
    "# drop observation with missing phase_worse_percentage_manual\n",
    "df = df.dropna(subset=['phase3_worse_percentage_manual'])\n",
    "# drop columns with 50% more missing values\n",
    "df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
    "# drop columns with all missing values\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# drop columns with all zero values\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "# Assuming 'df' is your dataset as a pandas DataFrame\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[['estimated_population','event_count_battles', 'event_count_explosions', 'event_count_violence',\n",
    "       'fatalities_battles', 'fatalities_explosions', 'fatalities_violence','nearest_neighbor_distance',\n",
    "       'GOSIF_GPP', 'rainfall_chirps', 'GOSIF_GPP_SD', \n",
    "        'temperature_2m_mean', 'temperature_2m_mean_sd',\n",
    "       'shortwave_radiation_sum', 'shortwave_radiation_sum_sd',\n",
    "       'precipitation_sum', 'precipitation_sum_sd','price_index']].values.astype(np.float32)\n",
    "y = df['phase3_plus_phase4'].values.astype(np.float32)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=53)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# impute X_train_scaled\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X_train_scaled)\n",
    "X_train_scaled = imp.transform(X_train_scaled)\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# impute X_test_scaled\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X_test_scaled)\n",
    "X_test_scaled = imp.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.011187633965191186\n",
      "Root Mean Squared Error: 0.10577161228416246\n",
      "Mean Absolute Error: 0.07834548102890003\n",
      "R-squared: 0.683700571038484\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_regressor.predict(X_test_scaled)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated_population: 0.16170885746534094\n",
      "event_count_battles: 0.015539730528102796\n",
      "event_count_explosions: 0.005961185503245342\n",
      "event_count_violence: 0.012126790205449\n",
      "fatalities_battles: 0.015680986178609902\n",
      "fatalities_explosions: 0.0046098171832989485\n",
      "fatalities_violence: 0.013381534232797521\n",
      "nearest_neighbor_distance: 0.05610880748445266\n",
      "GOSIF_GPP: 0.07344806907332554\n",
      "rainfall_chirps: 0.04741022597118192\n",
      "GOSIF_GPP_SD: 0.06639631894808014\n",
      "temperature_2m_mean: 0.08964808953750435\n",
      "temperature_2m_mean_sd: 0.1769367730707801\n",
      "shortwave_radiation_sum: 0.041359801434957155\n",
      "shortwave_radiation_sum_sd: 0.03875526182246983\n",
      "precipitation_sum: 0.030892507078744157\n",
      "precipitation_sum_sd: 0.032593269363811544\n",
      "price_index: 0.1174419749178481\n"
     ]
    }
   ],
   "source": [
    "# Obtain feature importances from the Random Forest model\n",
    "importances = rf_regressor.feature_importances_\n",
    "\n",
    "# Get the feature names from your dataset\n",
    "feature_names = ['estimated_population','event_count_battles', 'event_count_explosions', 'event_count_violence',\n",
    "       'fatalities_battles', 'fatalities_explosions', 'fatalities_violence','nearest_neighbor_distance',\n",
    "       'GOSIF_GPP', 'rainfall_chirps', 'GOSIF_GPP_SD', \n",
    "        'temperature_2m_mean', 'temperature_2m_mean_sd',\n",
    "       'shortwave_radiation_sum', 'shortwave_radiation_sum_sd',\n",
    "       'precipitation_sum', 'precipitation_sum_sd','price_index']\n",
    "\n",
    "# Print the feature importances\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
